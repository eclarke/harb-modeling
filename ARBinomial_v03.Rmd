---
title: "Modeling the effects of antibiotics on bacteria in the MIRA cohort"
author: "Erik Clarke"
date: "`r Sys.Date()`"
output:
  rmdformats::html_clean:
    highlight: kate
    code_folding: show
params:
  fig_fp: "figures"
  genus: "Staphylococcus"
  species: ""
  load_saved: TRUE
editor_options: 
  chunk_output_type: console
---

# Setup and preprocessing

```{r knitr_init, cache=F, echo=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(
  max.print="75", 
  tibble.print_max = 35)
opts_chunk$set(
  echo=TRUE,
  cache=TRUE,
  prompt=FALSE,
  comment=NA,
  message=FALSE,
  warning=FALSE,
  results="hide")
opts_knit$set(width=75)
```

```{r setup, cache=F, echo=TRUE}
library(here)
library(tidyverse); print(packageVersion("tidyverse"))
library(phyloseq)
library(bayesplot)
library(tidybayes)
library(ggbeeswarm)
library(magrittr)
library(tsibble)
library(rstan)
library(ggridges)
library(patchwork)
library(rethinking)
# library(kableExtra)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

fig_fp <- params$fig_fp
if (!dir.exists(here(fig_fp))) {
  dir.create(here(fig_fp))
}

source(here("shared_functions.R"))

mira_theme <- theme_bw() + theme(
  strip.background = element_rect(fill = NA)
)
theme_set(mira_theme)
```

We first load all the data into one object. 
This contains metadata related to each specimen, and the sequence and taxonomy table for all amplicon sequence variants (ASVs).

```{r load-data}
mira.all <- load_mira_data(
  seqtab_fp = here("../shared/data/seqtab.rds"),
  taxa_fp = here("../shared/data/taxa.rds"),
  meds_fp = here("../shared/data/MIRA_Medications_Table.csv"),
  specimen_fp = here("../shared/data/MIRA_Specimen_Table.csv")
)

seqs <- mira.all$seqs
meds <- mira.all$meds
```

Here we filter and preprocess the data. 
We first remove non-informative samples, merge duplicates, and restrict the data to the taxa of interest (_`r params$genus` `r params$species`_).
Next, we calculate our "antibiotic effectiveness window" for each subject and the antibiotic of interest (`r params$abx`).

```{r preprocess-data}
mira <- mira.all$ps
# Remove non-MIRA subjects from dataset (incl. blanks and controls)
.nonmira <- is.na(sample_data(mira)$subject_id)
print(sprintf("Removing %d non-MIRA samples...", sum(.nonmira)))
mira <- prune_samples(!.nonmira, mira)
# Remove culture samples (they break the subject/type/date unique constraint)
.culture <- grepl("Culture", sample_data(mira)$specimen_type)
print(sprintf("Removing %d culture samples...", sum(.culture)))
mira <- prune_samples(!.culture, mira)
# Remove empty samples
.empty <- sample_sums(mira) == 0
print(sprintf("Removing %d empty samples...", sum(.empty)))
mira <- prune_samples(!.empty, mira)

# Identify "duplicated" specimens (same subject, specimen type, and study day)
sample_data(mira) <- sample_data(mira) %>% 
  group_by(subject_id, specimen_type, study_day) %>%
  # "specimen_id3" has the same value for duplicated specimens so phyloseq can 
  # use it as a grouping level
  mutate(specimen_id3 = case_when(
    n() > 1 ~ paste0(first(as.character(specimen_id2)), "_D"),
    TRUE ~ as.character(specimen_id2)
  )) %>%
  ungroup() %>% as.data.frame() %>%
  set_rownames(.$specimen_id2)

# Sum abundances and merge sample table for duplicates
mira <- phyloseq::merge_samples(mira, "specimen_id3", fun=sum)
# Re-add the relevant metadata since merge_samples mangles factors and dates
sample_data(mira) <- sample_data(mira) %>% 
  mutate(specimen_id2 = rownames(.)) %>%
  select(specimen_id2, specimen_id3) %>%
  left_join(mira.all$samples) %>%
  ungroup() %>% as.data.frame() %>%
  set_rownames(.$specimen_id2)

# Restrict to only samples for which we have abx data
.abx_specimens <- as.character(inner_join(sample_data(mira), meds)$specimen_id2)
mira.abx <- prune_samples(
  sample_data(mira)$specimen_id2 %in% .abx_specimens, mira)

# Converted to melted form
agg <- phyloseq_to_agglomerated(mira.abx, "specimen_id2", "otu_id", "read_count")
d <- agg %>%
  # Calculate total reads
  group_by(specimen_id2) %>%
  mutate(total_reads = sum(read_count)) %>%
  ungroup() %>%
  # Collapse reads by genus/species
  filter(!is.na(Genus)) %>%
  group_by(specimen_id2, total_reads, Kingdom, Phylum, Class, Order, Family, Genus) %>%
  summarise(read_count = sum(read_count)) %>%
  ungroup() %>%
  filter(Genus == params$genus) %>%
  left_join(sample_data(mira.abx)) %>%
  select(-c(Kingdom:Family))

# Clean up subject/timepoint data
subjects <- sample_data(mira.abx) %>%
  group_by(subject_id) %>% 
  mutate(exit_date = max(collection_date)) %>%
  distinct(subject_id, enroll_date, exit_date) %>%
  right_join(meds) %>%
  group_by(subject_id) %>%
  mutate(study_day = as.integer(collection_date - enroll_date)) %>%
  mutate(exit_day = study_day[collection_date == exit_date]) %>%
  # Limit to only a week before enrollment date and nothing after
  filter(study_day > -3, collection_date <= exit_date)

# Manually split MIRA_024 into two sub-subjects
subjects <- subjects %>%
  mutate(subject_id2 = case_when(
    subject_id != "MIRA_024" ~ subject_id,
    study_day <= 33 ~ "MIRA_024a",
    study_day >= 73 ~ "MIRA_024b",
    TRUE ~ NA_character_
  )) %>%
  filter(!is.na(subject_id2)) %>%
  mutate(study_day = case_when(
    subject_id2 == "MIRA_024b" ~ study_day - 73,
    TRUE ~ as.double(study_day)
  )) %>%
  mutate(exit_day = ifelse(subject_id2 == "MIRA_024b", exit_day-73, exit_day))

# Clean up antibiotics data
subjects_abx <- subjects %>%
  ungroup() %>%
  separate_rows(abx_b) %>%
  # Handle coformulations
  mutate(abx_b2 = case_when(
    abx_b == "clavulanate" ~ "amoxicillin.clavulanate",
    abx_b == "sulbactam" ~ "ampicillin.sulbactam",
    abx_b == "tazobactam" ~ "piperacillin.tazobactam",
    abx_b == "trimethoprim" ~ "trimethoprim.sulfamethoxazole",
    TRUE ~ abx_b)) %>%
  group_by(subject_id, collection_date) %>%
  # Remove remaining part of coformulation if coformulation is present
  filter(
    (abx_b2 != "amoxicillin" & ("amoxicillin.clavulanate" %in% abx_b2)) | 
    !("amoxicillin.clavulanate" %in% abx_b2) |
    is.na(abx_b2)
  ) %>%
  filter(
    (abx_b2 != "ampicillin" & ("ampicillin.sulbactam" %in% abx_b2)) |
    !("ampicillin.sulbactam" %in% abx_b2) |
    is.na(abx_b2)
  ) %>%
  filter(
    (abx_b2 != "sulfamethoxazole" & ("trimethoprim.sulfamethoxazole" %in% abx_b2)) |
    !("trimethoprim.sulfamethoxazole" %in% abx_b2) |
    is.na(abx_b2)
  ) %>%
  ungroup() %>%
  filter((abx_b2 != "piperacillin") | is.na(abx_b2)) %>%
  mutate(abx_b = abx_b2) %>%
  select(-abx_b2) %>%
  mutate(abx_idx = as.numeric(as.factor(abx_b))) %>%
  mutate(abx_yn = ifelse(is.na(abx_b), 0, 1))

## RETURN TO THIS LATER: ABX EFFECTIVENESS WINDOWS
# subjects %>%
#   mutate(abx_yn = grepl(params$abx, abx_b)) %>%
#   # the .size parameter here is how long it takes to reach peak
#   # 1 = that day
#   mutate(reached_peak = slide_lgl(abx_yn, all, .size=1)) %>%
#   # the lag parameter here defines how long it lasts after end of admin.
#   mutate(on_abx = effective_window(reached_peak, lag=1)) %>%
#   ungroup()
  
d2 <- left_join(subjects_abx, d)

saveRDS(d2, with(params, sprintf("_%s_%s_allabx.rds", genus, species)))
```

# Model v2.0 {#v2-0}

The primary distinction for model 2.0 is modeling multiple antibiotics together. 

## Model v2.0.0

There is no time dependence or subject-specific intercepts for this first iteration of the model.

The Stan code for the model:
```{r results="show"}
writeLines(readLines("arb_2.0.0.stan"))
```

### Fitting

First, let's create dummy data using the generative process described by the model so we can see if this works at all.

```{r}
sim200.generator <- function(days_per_subject, n_subjects, n_abx, a, s_abx, b_abx) {
  stopifnot(length(s_abx) == n_abx)
  stopifnot(ncol(b_abx) == n_abx)
  stopifnot(nrow(b_abx) == n_subjects)
  
  n_timepoints <- n_subjects * days_per_subject
  ii_subjects <- as.vector(sapply(1:n_subjects, function(x) rep(x, days_per_subject)))
  abx <- t(matrix(as.integer(rbernoulli(n_abx*n_timepoints)), nrow=n_abx, ncol=n_timepoints))
  total <- rep(1000, n_timepoints)

  phi <- rep(NA, n_timepoints)
  for (i in 1:n_timepoints) {
    phi[i] = a
    for (j in 1:n_abx) {
      phi[i] = phi[i] + b_abx[ii_subjects[i], j] * abx[i, j]
    }
  }
  
  # Generate and return simulated
  reads <- rbinom(n_timepoints, total, logistic(phi))
  data <- list(
    n_timepoints = n_timepoints,
    n_subjects = n_subjects,
    n_abx = n_abx,
    ii_subjects = ii_subjects,
    abx = abx,
    total = total,
    reads = reads
  )
  params <- list(
    a = a,
    s_abx = s_abx,
    b_abx = b_abx
  )
  list(data=data, params=params)
}

dsim200 <- with(data.frame(), {
  n_subjects <- 6
  n_abx <- 5
  a <- 1
  s_abx <- rexp(n_abx, 1)
  b_abx <- t(matrix(rnorm(n_subjects*n_abx, mean=0, sd=s_abx), nrow=n_abx, ncol=n_subjects))
  sim200.generator(10, n_subjects, n_abx, a, s_abx, b_abx)
})
```

Fit the model:

```{r}
m200 <- stan(
  file="arb_2.0.0.stan", data=dsim200$data, cores=4, 
  control=list(max_treedepth=12, adapt_delta=0.9))
```

### Diagnostics

Check a few of the chains:
```{r}
bayesplot::mcmc_trace(
  rstan::extract(m200, inc_warmup=T, permuted=F), 
  pars=c("a", "b_abx[1,1]", "s_abx[1]"))
```

### Concordance

How well do the predicted values match up with the true values?

```{r fig.height=5, fig.width=7}
m200 %>%
  spread_samples(a, b_abx[subject, abx], s_abx[abx]) %>%
  ggplot(aes(x = b_abx, y=fct_rev(as.factor(subject)), fill=0.5-abs(0.5-..ecdf..))) +
  stat_density_ridges(geom="density_ridges_gradient", calc_ecdf=TRUE, scale=1.5) +
  geom_vline(aes(xintercept=0), linetype=2, alpha=0.4) +
  scale_fill_viridis_c("Probability", direction=1, option="C") +
  facet_wrap(vars(abx), scales="free")
```

One thing I noticed here is that large values of `s_abx`, resulting in wide magnitude ranges of `b_abx`, tend to get compressed by the transformation to the unit scale by the 'logistic' function. 
When this happens, we are unable to correctly recaptulate the values of `s_abx` and `b_abx` as they have gotten "lost in the extremes" and aren't recoverable from the data (i.e. the probabilities in the binomial process just go to 1 or 0).

During the generative process, I had the administration days (the `abx` matrix) following a set pattern of abx co-administration. 
This resulted in the inability to retrieve the `b_abx` values, I think due to collinearity. 
It suggests that I should find a diagnostic for collinearity that I can use on real-world data. 
Once I switched the generative process to effectively randomize administration days with a yes/no value of 0.5, it retrieved the `b_abx` values nearly perfectly.

## Model v2.0.1

Here we will add subject-specific intercepts.

```{r results="show"}
writeLines(readLines("arb_2.0.1.stan"))
```

### Fitting

Again, create simulated data to test model integrity.

```{r}
sim201.generator <- function(days_per_subject, n_subjects, n_abx, a, a_subj, s_subj, s_abx, b_abx) {
  stopifnot(length(a_subj) == n_subjects)
  stopifnot(length(s_abx) == n_abx)
  stopifnot(ncol(b_abx) == n_abx)
  stopifnot(nrow(b_abx) == n_subjects)
  
  n_timepoints <- n_subjects * days_per_subject
  ii_subjects <- as.vector(sapply(1:n_subjects, function(x) rep(x, days_per_subject)))
  abx <- t(matrix(as.integer(rbernoulli(n_abx*n_timepoints)), nrow=n_abx, ncol=n_timepoints))
  total <- rep(1000, n_timepoints)

  phi <- rep(NA, n_timepoints)
  for (i in 1:n_timepoints) {
    phi[i] = a + a_subj[ii_subjects[i]]
    for (j in 1:n_abx) {
      phi[i] = phi[i] + b_abx[ii_subjects[i], j] * abx[i, j]
    }
  }
  
  # Generate and return simulated
  reads <- rbinom(n_timepoints, total, logistic(phi))
  data <- list(
    n_timepoints = n_timepoints,
    n_subjects = n_subjects,
    n_abx = n_abx,
    ii_subjects = ii_subjects,
    abx = abx,
    total = total,
    reads = reads
  )
  params <- list(
    a = a,
    a_subj = a_subj,
    s_subj = s_subj,
    s_abx = s_abx,
    b_abx = b_abx
  )
  list(data=data, params=params)
}

dsim201 <- with(data.frame(), {
  n_subjects <- 6
  n_abx <- 5
  a <- 1
  s_abx <- rexp(n_abx, 1)
  s_subj <- rexp(1, 1)
  a_subj <- rnorm(n_subjects, 0, s_subj);
  b_abx <- t(matrix(rnorm(n_subjects*n_abx, mean=0, sd=s_abx), nrow=n_abx, ncol=n_subjects))
  sim201.generator(10, n_subjects, n_abx, a, a_subj, s_subj, s_abx, b_abx)
})
```


Fit the model:

```{r}
m201 <- stan(
  file="arb_2.0.1.stan", data=dsim201$data, cores=4, 
  control=list(max_treedepth=12, adapt_delta=0.9))
```

### Diagnostics

Check a few of the chains:
```{r}
bayesplot::mcmc_trace(
  rstan::extract(m201, inc_warmup=T, permuted=F), 
  pars=c("a", "b_abx[1,1]", "s_abx[1]"))
```

### Concordance

How well do the predicted values match up with the true values?

```{r fig.height=5, fig.width=7}
m201 %>%
  spread_samples(a, b_abx[subject, abx], s_abx[abx]) %>%
  ggplot(aes(x = b_abx, y=fct_rev(as.factor(subject)), fill=0.5-abs(0.5-..ecdf..))) +
  stat_density_ridges(geom="density_ridges_gradient", calc_ecdf=TRUE, scale=1.5) +
  geom_vline(aes(xintercept=0), linetype=2, alpha=0.4) +
  scale_fill_viridis_c("Probability", direction=1, option="C") +
  facet_wrap(vars(abx), scales="free")
```

```{r}
m201 %>% 
  spread_samples(a, a_subj[subject]) %>%
  ggplot(aes(x = a_subj, y=fct_rev(as.factor(subject)))) +
  tidybayes::stat_intervalh()
```

This model also successfully recapitulates the true parameter values.

## Model v2.0.2

Let's add back the time dependence, both with a separate `b_lag` term as well as multipliers on the antibiotic terms.


```{r results="show"}
writeLines(readLines("arb_2.0.2.stan"))
```

### Fitting

Again, create simulated data to test model integrity.

```{r}
sim202.generator <- function(days_per_subject, n_subjects, n_abx, a, a_subj, s_subj, s_abx, b_abx, b_lag) {
  stopifnot(length(a_subj) == n_subjects)
  stopifnot(length(s_abx) == n_abx)
  stopifnot(ncol(b_abx) == n_abx)
  stopifnot(nrow(b_abx) == n_subjects)
  
  n_timepoints <- n_subjects * days_per_subject
  ii_subjects <- as.vector(sapply(1:n_subjects, function(x) rep(x, days_per_subject)))
  abx <- t(matrix(as.integer(rbernoulli(n_abx*n_timepoints, p=0.2)), nrow=n_abx, ncol=n_timepoints))
  total <- rep(1000, n_timepoints)

  phi <- rep(NA, n_timepoints)
  prev <- rep(0.5, n_timepoints)
  reads <- rep(NA, n_timepoints)
  prev <- rep(NA, n_timepoints)
  prev[c(1, c(1:n_subjects)*days_per_subject)] <- 0.5
  
  for (i in 1:n_timepoints) {
    if (is.na(prev[i])) {
      prev[i] <- reads[i-1]/total[i-1]
    }
    phi[i] <- a + a_subj[ii_subjects[i]] + b_lag * prev[i]
    for (j in 1:n_abx) {
      phi[i] <- phi[i] + b_abx[ii_subjects[i], j] * abx[i, j] * prev[i]
    }
    reads[i] <- rbinom(1, total[i], logistic(phi[i]))
  }
  
  data <- list(
    n_timepoints = n_timepoints,
    n_subjects = n_subjects,
    n_abx = n_abx,
    ii_subjects = ii_subjects,
    abx = abx,
    total = total,
    reads = reads,
    prev = prev
  )
  
  params <- list(
    a = a,
    a_subj = a_subj,
    s_subj = s_subj,
    s_abx = s_abx,
    b_lag = b_lag,
    b_abx = b_abx
  )
  list(data=data, params=params)
}
set.seed(16)
dsim202 <- with(data.frame(), {
  n_subjects <- 6
  n_abx <- 5
  a <- 0.2
  s_abx <- rexp(n_abx, 1)
  s_subj <- rexp(1, 1)
  a_subj <- rnorm(n_subjects, 0, s_subj);
  b_abx <- t(matrix(rnorm(n_subjects*n_abx, mean=0, sd=s_abx), nrow=n_abx, ncol=n_subjects))
  b_lag <- 0.5
  sim202.generator(20, n_subjects, n_abx, a, a_subj, s_subj, s_abx, b_abx, b_lag)
})
```

Fit the model:

```{r}
m202 <- stan(
  file="arb_2.0.2.stan", data=dsim202$data, cores=4, 
  control=list(max_treedepth=12, adapt_delta=0.9))
```

There were no errors or warnings fitting the model.

### Concordance
```{r}
m202 %>% 
  spread_samples(a, b_lag, a_subj[subject]) %>%
  median_qi()
  ggplot(aes(x = a_subj, y=fct_rev(as.factor(subject)))) +
  tidybayes::stat_intervalh()
```

```{r} 
m202 %>%
  spread_samples(a, b_abx[subject, abx], s_abx[abx]) %>%
  ggplot(aes(x = b_abx, y=fct_rev(as.factor(subject)), fill=0.5-abs(0.5-..ecdf..))) +
  stat_density_ridges(geom="density_ridges_gradient", calc_ecdf=TRUE, scale=1.5) +
  geom_vline(aes(xintercept=0), linetype=2, alpha=0.4) +
  scale_fill_viridis_c("Probability", direction=1, option="C") +
  facet_wrap(vars(abx), scales="free")
```

This model successfully recapitulates the given parameters as well.

## Model 2.0.3

In model 1.5.4, we had two antibiotics terms- the first was an interaction with the presence/absence of the abx and the proportion of bacteria, while the second was just presence/absence of abx. 
This helped capture the potentially positive effect of the antibiotic (e.g. increased chance of seeing the bacteria) even if the bacteria was not detected previously.

```{r results="show"}
writeLines(readLines("arb_2.0.3.stan"))
```

### Fitting

Again, create simulated data to test model integrity.

```{r}
sim203.generator <- function(
  days_per_subject, n_subjects, n_abx, 
  a, a_subj, s_subj, 
  s_abx, b_abx, 
  s_abxp, b_abxp,
  b_lag) 
{
  stopifnot(length(a_subj) == n_subjects)
  stopifnot(length(s_abx) == n_abx)
  stopifnot(length(s_abxp) == n_abx)
  stopifnot(ncol(b_abx) == n_abx)
  stopifnot(nrow(b_abx) == n_subjects)
  stopifnot(nrow(b_abxp) == n_subjects)
  
  n_timepoints <- n_subjects * days_per_subject
  ii_subjects <- as.vector(sapply(1:n_subjects, function(x) rep(x, days_per_subject)))
  abx <- t(matrix(as.integer(rbernoulli(n_abx*n_timepoints)), nrow=n_abx, ncol=n_timepoints))
  total <- rep(1000, n_timepoints)

  phi <- rep(NA, n_timepoints)
  prev <- rep(0.5, n_timepoints)
  reads <- rep(NA, n_timepoints)
  prev <- rep(NA, n_timepoints)
  prev[c(1, c(1:n_subjects)*days_per_subject)] <- 0.5
  
  for (i in 1:n_timepoints) {
    if (is.na(prev[i])) {
      prev[i] <- reads[i-1]/total[i-1]
    }
    phi[i] <- a + a_subj[ii_subjects[i]] + b_lag * prev[i]
    for (j in 1:n_abx) {
      phi[i] <- (
        phi[i] + 
          b_abxp[ii_subjects[i], j] * abx[i, j] * prev[i] + 
          b_abx[ii_subjects[i], j] * abx[i,j]
      )
    }
    reads[i] <- rbinom(1, total[i], logistic(phi[i]))
  }
  
  data <- list(
    n_timepoints = n_timepoints,
    n_subjects = n_subjects,
    n_abx = n_abx,
    ii_subjects = ii_subjects,
    abx = abx,
    total = total,
    reads = reads,
    prev = prev
  )
  
  params <- list(
    a = a,
    a_subj = a_subj,
    s_subj = s_subj,
    s_abx = s_abx,
    s_abxp = s_abxp,
    b_lag = b_lag,
    b_abx = b_abx,
    b_abxp = b_abxp
  )
  list(data=data, params=params)
}
set.seed(32)
dsim203 <- with(data.frame(), {
  n_subjects <- 6
  n_abx <- 5
  a <- 0.2
  s_abx <- rexp(n_abx, 1)
  s_abxp <- rexp(n_abx, 1)
  s_subj <- rexp(1, 1)
  a_subj <- rnorm(n_subjects, 0, s_subj);
  b_abx <- t(matrix(rnorm(n_subjects*n_abx, mean=0, sd=s_abx), nrow=n_abx, ncol=n_subjects))
  b_abxp <- t(matrix(rnorm(n_subjects*n_abx, mean=0, sd=s_abxp), nrow=n_abx, ncol=n_subjects))
  b_lag <- 0.5
  sim203.generator(10, n_subjects, n_abx, a, a_subj, s_subj, s_abx, b_abx, s_abxp, b_abxp, b_lag)
})
```

Fit the model:

```{r}
m203 <- stan(
  file="arb_2.0.3.stan", data=dsim203$data, cores=4, 
  control=list(max_treedepth=15, adapt_delta=0.99))
```

There were no errors or warnings fitting the model.

### Concordance

```{r}
invisible(with(data.frame(), {
  # browser()
  true_b_abx <- reshape2::melt(dsim203$params$b_abx, varnames=c("subject", "abx"))
  p <- m203 %>%
    spread_samples(b_abx[subject, abx]) %>%
    ggplot(aes(x = b_abx, y=fct_rev(as.factor(subject)), fill=0.5-abs(0.5-..ecdf..))) +
    stat_density_ridges(geom="density_ridges_gradient", calc_ecdf=TRUE, scale=1) +
    geom_vline(aes(xintercept=0), linetype=2, alpha=0.4) +
    geom_point(data=true_b_abx, aes(x=value, fill=NULL, color="True value")) +
    scale_fill_viridis_c("Probability", direction=1, option="C") +
    scale_color_manual("", values=c("True value"="dodgerblue")) +
    facet_wrap(vars(abx), scales="free") +
    ggtitle("Estimates vs true values of 'b_abx'")
  plot(p)
}))
```

```{r}
invisible(with(data.frame(), {
  # browser()
  true_b_abxp <- reshape2::melt(dsim203$params$b_abxp, varnames=c("subject", "abx"))
  p <- m203 %>%
    spread_samples(b_abxp[subject, abx]) %>%
    ggplot(aes(x = b_abxp, y=fct_rev(as.factor(subject)), fill=0.5-abs(0.5-..ecdf..))) +
    stat_density_ridges(geom="density_ridges_gradient", calc_ecdf=TRUE, scale=1) +
    geom_vline(aes(xintercept=0), linetype=2, alpha=0.4) +
    geom_point(data=true_b_abxp, aes(x=value, fill=NULL, color="True value")) +
    scale_fill_viridis_c("Probability", direction=1, option="C") +
    scale_color_manual("", values=c("True value"="dodgerblue")) +
    facet_wrap(vars(abx), scales="free") +
    ggtitle("Estimates vs true values of 'b_abxp'")
  plot(p)
}))
```

```{r}
invisible(with(data.frame(), {
  # browser()
  true_a_subj <- data.frame(value=dsim203$params$a_subj)
  true_a_subj$subject <- 1:nrow(true_a_subj)
  p <- m203 %>%
    spread_samples(a_subj[subject]) %>%
    ggplot(aes(x = a_subj, y=fct_rev(as.factor(subject)), fill=0.5-abs(0.5-..ecdf..))) +
    stat_density_ridges(geom="density_ridges_gradient", calc_ecdf=TRUE, scale=1) +
    geom_vline(aes(xintercept=0), linetype=2, alpha=0.4) +
    geom_point(data=true_a_subj, aes(x=value, fill=NULL, color="True value")) +
    scale_fill_viridis_c("Probability", direction=1, option="C") +
    scale_color_manual("", values=c("True value"="dodgerblue")) +
    ggtitle("Estimates vs true values of 'a_subj'")
  plot(p)
}))
```

In some situations the parameters are close but it doesn't consistently reproduce the true values.
I thought it might be due to the two parameters but if I remove one of them it doesn't recapitulate them any better.
Especially problematic are the subject-specific intercepts, which is likely due to the partial pooling.
If I make them completely independent, I can recapture the true parameters.

This strongly suggests to me that the partial pooling is drawing the estimated values away from the true values in some cases.

What features are missing?

- specimen-specific intercepts

## Model v2.0.4

In which we fit specimen-specific intercepts to model overdispersion.

```{r results="show"}
writeLines(readLines("arb_2.0.4.stan"))
```

### Fitting

We won't simulate new data for this, but instead fit to our previous data.

```{r}
m204 <- stan(
  file="arb_2.0.4.stan", data=dsim203$data, cores=4, 
  control=list(max_treedepth=15, adapt_delta=0.99))
```

### Diagnostics

```{r}
s204 <- summary(m204)$summary %>% as.data.frame() %>% 
  rownames_to_column("parameter") %>% as.tibble()
s204 %>% filter(Rhat >= 1.1 | n_eff < 200) %>% kable()
```

```{r}
pairs(
  m204,
  pars = c(
    "a",
    "s_spec_nc",
    "a_spec[1]",
    "s_subj_nc",
    "a_subj_nc",
    "lp__"))
```

### Concordance

```{r}
invisible(with(data.frame(), {
  # browser()
  true_b_abx <- reshape2::melt(dsim203$params$b_abx, varnames=c("subject", "abx"))
  p <- m204 %>%
    spread_samples(b_abx[subject, abx]) %>%
    ggplot(aes(x = b_abx, y=fct_rev(as.factor(subject)), fill=0.5-abs(0.5-..ecdf..))) +
    stat_density_ridges(geom="density_ridges_gradient", calc_ecdf=TRUE, scale=1) +
    geom_vline(aes(xintercept=0), linetype=2, alpha=0.4) +
    geom_point(data=true_b_abx, aes(x=value, fill=NULL, color="True value")) +
    scale_fill_viridis_c("Probability", direction=1, option="C") +
    scale_color_manual("", values=c("True value"="dodgerblue")) +
    facet_wrap(vars(abx), scales="free") +
    ggtitle("Estimates vs true values of 'b_abx'")
  plot(p)
}))
```

```{r}
invisible(with(data.frame(), {
  # browser()
  true_b_abxp <- reshape2::melt(dsim203$params$b_abxp, varnames=c("subject", "abx"))
  p <- m204 %>%
    spread_samples(b_abxp[subject, abx]) %>%
    ggplot(aes(x = b_abxp, y=fct_rev(as.factor(subject)), fill=0.5-abs(0.5-..ecdf..))) +
    stat_density_ridges(geom="density_ridges_gradient", calc_ecdf=TRUE, scale=1) +
    geom_vline(aes(xintercept=0), linetype=2, alpha=0.4) +
    geom_point(data=true_b_abxp, aes(x=value, fill=NULL, color="True value")) +
    scale_fill_viridis_c("Probability", direction=1, option="C") +
    scale_color_manual("", values=c("True value"="dodgerblue")) +
    facet_wrap(vars(abx), scales="free") +
    ggtitle("Estimates vs true values of 'b_abxp'")
  plot(p)
}))
```

```{r}
invisible(with(data.frame(), {
  # browser()
  true_a_subj <- data.frame(value=dsim203$params$a_subj)
  true_a_subj$subject <- 1:nrow(true_a_subj)
  p <- m204 %>%
    spread_samples(a_subj[subject]) %>%
    ggplot(aes(x = a_subj, y=fct_rev(as.factor(subject)), fill=0.5-abs(0.5-..ecdf..))) +
    stat_density_ridges(geom="density_ridges_gradient", calc_ecdf=TRUE, scale=1) +
    geom_vline(aes(xintercept=0), linetype=2, alpha=0.4) +
    geom_point(data=true_a_subj, aes(x=value, fill=NULL, color="True value")) +
    scale_fill_viridis_c("Probability", direction=1, option="C") +
    scale_color_manual("", values=c("True value"="dodgerblue")) +
    ggtitle("Estimates vs true values of 'a_subj'")
  plot(p)
}))
```

The addition of specimen intercepts does not appear to seriously change the estimates of the other parameters, which is good.
It was necessary to reformulate the `a_spec` parameter to non-centered parameterization for efficient fitting (and even then I encountered 2 divergent transitions).
I had to move both `a_spec` and `a_subj` to NCP to eliminate divergences entirely.

## Model v2.0.5

The last concern to address is situations where we're modeling a `b_abx` combo for a subject that never got that antibiotic, ever.

The question is whether estimating these `b_abx` terms impedes identifiability for the other `b_abx` terms for which we do have data.
If it doesn't, then these are basically harmless and can be ignored.
If these 'nonexistent' `b_abx` terms affect the others, we need to find a way to "remove" them from consideration.

One of the things we could do is assign different priors to those terms: instead of having a shared variance/intercept term (partial pooling), these are on their own, i.e. not multiplied by `s_abx_nc`. 

First, let's see what happens when we simulate a patient that never got an antibiotic:

```{r}
sim205.generator <- function(
  days_per_subject, n_subjects, n_abx, 
  a, a_subj, s_subj, 
  s_abx, b_abx, 
  s_abxp, b_abxp,
  b_lag, zero_subject_1=T) 
{
  stopifnot(length(a_subj) == n_subjects)
  stopifnot(length(s_abx) == n_abx)
  stopifnot(length(s_abxp) == n_abx)
  stopifnot(ncol(b_abx) == n_abx)
  stopifnot(nrow(b_abx) == n_subjects)
  stopifnot(nrow(b_abxp) == n_subjects)
  
  n_timepoints <- n_subjects * days_per_subject
  ii_subjects <- as.vector(sapply(1:n_subjects, function(x) rep(x, days_per_subject)))
  abx <- t(matrix(as.integer(rbernoulli(n_abx*n_timepoints)), nrow=n_abx, ncol=n_timepoints))
  # Zero out one subject's administration of an single abx entirely (subject 1, abx 1)
  if (zero_subject_1) {
    abx[(1:days_per_subject), 1] <- 0    
  }

  total <- rep(1000, n_timepoints)

  phi <- rep(NA, n_timepoints)
  prev <- rep(0.5, n_timepoints)
  reads <- rep(NA, n_timepoints)
  prev <- rep(NA, n_timepoints)
  prev[c(1, c(1:n_subjects)*days_per_subject)] <- 0.5    

  set.seed(2000)
  for (i in 1:n_timepoints) {
    if (is.na(prev[i])) {
      prev[i] <- reads[i-1]/total[i-1]
    }
    phi[i] <- a + a_subj[ii_subjects[i]] + b_lag * prev[i]
    for (j in 1:n_abx) {
      phi[i] <- (
        phi[i] + 
          b_abxp[ii_subjects[i], j] * abx[i, j] * prev[i] + 
          b_abx[ii_subjects[i], j] * abx[i,j]
      )
    }
    reads[i] <- rbinom(1, total[i], logistic(phi[i]))
  }
  
  data <- list(
    n_timepoints = n_timepoints,
    n_subjects = n_subjects,
    n_abx = n_abx,
    ii_subjects = ii_subjects,
    abx = abx,
    total = total,
    reads = reads,
    prev = prev
  )
  
  params <- list(
    a = a,
    a_subj = a_subj,
    s_subj = s_subj,
    s_abx = s_abx,
    s_abxp = s_abxp,
    b_lag = b_lag,
    b_abx = b_abx,
    b_abxp = b_abxp
  )
  list(data=data, params=params)
}
set.seed(32)
dsim205 <- with(data.frame(), {
  n_subjects <- 6
  n_abx <- 5
  a <- 0.2
  s_abx <- rexp(n_abx, 1)
  s_abxp <- rexp(n_abx, 1)
  s_subj <- rexp(1, 1)
  a_subj <- rnorm(n_subjects, 0, s_subj);
  b_abx <- t(matrix(rnorm(n_subjects*n_abx, mean=0, sd=s_abx), nrow=n_abx, ncol=n_subjects))
  b_abxp <- t(matrix(rnorm(n_subjects*n_abx, mean=0, sd=s_abxp), nrow=n_abx, ncol=n_subjects))
  b_abx[1,1] <- 1
  b_abxp[1,1] <- 1
  b_lag <- 0.5
  sim205.generator(10, n_subjects, n_abx, a, a_subj, s_subj, s_abx, b_abx, s_abxp, b_abxp, b_lag, F)
})
set.seed(32)
dsim205.2 <- with(data.frame(), {
  n_subjects <- 6
  n_abx <- 5
  a <- 0.2
  s_abx <- rexp(n_abx, 1)
  s_abxp <- rexp(n_abx, 1)
  s_subj <- rexp(1, 1)
  a_subj <- rnorm(n_subjects, 0, s_subj);
  b_abx <- t(matrix(rnorm(n_subjects*n_abx, mean=0, sd=s_abx), nrow=n_abx, ncol=n_subjects))
  b_abxp <- t(matrix(rnorm(n_subjects*n_abx, mean=0, sd=s_abxp), nrow=n_abx, ncol=n_subjects))
  b_abx[1,1] <- 1
  b_abxp[1,1] <- 1
  b_lag <- 0.5
  sim205.generator(10, n_subjects, n_abx, a, a_subj, s_subj, s_abx, b_abx, s_abxp, b_abxp, b_lag, T)
})
dsim205.2$data$reads[11:60] <- dsim205$data$reads[11:60]
dsim205.2$data$prev[11:60] <- dsim205$data$prev[11:60]
```

### Fitting (v2.0.5.1)

```{r}
set.seed(123)
m205.1 <- stan(
  file="arb_2.0.4.stan", data=dsim205$data, cores=4, 
  control=list(max_treedepth=15, adapt_delta=0.99), seed=123)
```

```{r}
invisible(with(data.frame(), {
  # browser()
  true_b_abxp <- reshape2::melt(dsim205$params$b_abxp, varnames=c("subject", "abx"))
  p <- m205.1 %>%
    spread_samples(b_abxp[subject, abx]) %>%
    ggplot(aes(x = b_abxp, y=fct_rev(as.factor(subject)), fill=0.5-abs(0.5-..ecdf..))) +
    stat_density_ridges(geom="density_ridges_gradient", calc_ecdf=TRUE, scale=1) +
    geom_vline(aes(xintercept=0), linetype=2, alpha=0.4) +
    geom_point(data=true_b_abxp, aes(x=value, fill=NULL, color="True value")) +
    scale_fill_viridis_c("Probability", direction=1, option="C") +
    scale_color_manual("", values=c("True value"="dodgerblue")) +
    facet_wrap(vars(abx), scales="free") +
    ggtitle("Estimates vs true values of 'b_abxp' (v1)")
  plot(p)
}))
```

### Fitting (v2.0.5.2)

```{r}
set.seed(123)
m205.2 <- stan(
  file="arb_2.0.4.stan", data=dsim205.2$data, cores=4, 
  control=list(max_treedepth=15, adapt_delta=0.99), seed=123)
```

```{r}
invisible(with(data.frame(), {
  # browser()
  true_b_abxp <- reshape2::melt(dsim205.2$params$b_abxp, varnames=c("subject", "abx"))
  p <- m205.2 %>%
    spread_samples(b_abxp[subject, abx]) %>%
    ggplot(aes(x = b_abxp, y=fct_rev(as.factor(subject)), fill=0.5-abs(0.5-..ecdf..))) +
    stat_density_ridges(geom="density_ridges_gradient", calc_ecdf=TRUE, scale=1) +
    geom_vline(aes(xintercept=0), linetype=2, alpha=0.4) +
    geom_point(data=true_b_abxp, aes(x=value, fill=NULL, color="True value")) +
    scale_fill_viridis_c("Probability", direction=1, option="C") +
    scale_color_manual("", values=c("True value"="dodgerblue")) +
    facet_wrap(vars(abx), scales="free") +
    ggtitle("Estimates vs true values of 'b_abxp' (v2)")
  plot(p)
}))
```

It seems it does slightly affect the other parameters, but I do not want to address this currently due to the complexity of assigning different priors to different betas.

### Fitting (real data)

```{r}
dat205.generator <- function(d2, .specimen_type, min_times_abx=1) {
  d3 <- d2 %>% filter(specimen_type==.specimen_type) %>%
    distinct(subject_id, specimen_id2, read_count, total_reads, study_day) %>%
    group_by(subject_id) %>%
    arrange(subject_id, study_day) %>%
    mutate(prev = lag(read_count)/lag(total_reads)) %>%
    right_join(d2) %>%
    filter(!is.na(prev)) %>%
    rename(reads=read_count, total=total_reads, specimens=specimen_id2, subjects=subject_id) %>%
    mutate(abx_b = ifelse(is.na(abx_b), "none", abx_b))
  # browser()
  abx_tally <- d3 %>% group_by(abx_b) %>% 
    summarize(n_subjects = n_distinct(subjects), n_admins=n())
  abx_to_keep <- filter(abx_tally, n_subjects > min_times_abx, abx_b != "none")$abx_b
  subjects_to_keep <- unique((filter(d3, abx_b %in% abx_to_keep) %>% group_by(subjects) %>% 
                         filter(n_distinct(abx_b) == length(abx_to_keep)))$subjects)
  stopifnot(length(subjects_to_keep) > 0)
  # subjects_to_keep <- (d3 %>% filter(abx_b %in% abx_to_keep) %>% distinct(subjects))$subjects
  
  d4 <- d3 %>% ungroup() %>%
    filter(subjects %in% subjects_to_keep)
  d_abx <- d4 %>%
    reshape2::dcast(subjects + specimens + reads + total + prev ~ abx_b, value.var="abx_yn", fill=0)
  # browser()
  result <- tidybayes::compose_data(d_abx[, c(1:5)])
  abx <- as.matrix(d_abx[, -c(1:5)])
  # Remove abxs seen less than the minimum, and the NA column
  abx <- abx[, abx_to_keep, drop=F]
  result$abx_b <- as.numeric(as.factor(colnames(abx)))
  result$n_abx <- ncol(abx)
  result$abx <- abx
  result$d4 <- d4
  result
}

d205.3 <- dat205.generator(d2, "Sputum")

# Correlation checking - come back later
# abx.cor <- cor(abx)
# diag(abx.cor) <- NA
# abx.cor[lower.tri(abx.cor)] <- NA
# abx.cor <- reshape2::melt(abx.cor)

m205.3 <- stan(
  file="arb_2.0.5.stan", data=d205.3, cores=4, iter=1000, chains=4,
  control=list(max_treedepth=17, adapt_delta=0.99))
saveRDS(m205.3, file="fit_arb_2.0.5_sputum.rds")
```

```{r}
bayesplot::mcmc_trace(
  rstan::extract(m205.3, inc_warmup=T, permuted=F), 
  pars=c("a", "b_abx[1,1]", "s_abx[1]"))
```

```{r}
s205.sp <- summary(m205.3)$summary %>% as.data.frame() %>% 
  rownames_to_column("parameter") %>% as.tibble()
s205.sp %>% filter(Rhat >= 1.1) %>% kable()
```

There were 11 divergent transitions, which I'm unclear on how to remove.

## Model v2.0.6

I want to explore ways of simulating and fixing the divergent transitions.

### Fitting (simulated)
```{r}
sim206.generator <- function(
  days_per_subject, n_subjects, n_abx, 
  a, a_subj, s_subj, 
  s_abx, b_abx, 
  s_abxp, b_abxp,
  b_lag, abx_prob=0.2, sparsity=0.75) 
{
  stopifnot(length(a_subj) == n_subjects)
  stopifnot(length(s_abx) == n_abx)
  stopifnot(length(s_abxp) == n_abx)
  stopifnot(ncol(b_abx) == n_abx)
  stopifnot(nrow(b_abx) == n_subjects)
  stopifnot(nrow(b_abxp) == n_subjects)
  
  n_timepoints <- n_subjects * days_per_subject
  ii_subjects <- as.vector(sapply(1:n_subjects, function(x) rep(x, days_per_subject)))
  abx <- t(matrix(
    as.integer(rbernoulli(n_abx*n_timepoints, p=abx_prob)), 
    nrow=n_abx, ncol=n_timepoints))

  total <- rep(1000, n_timepoints)

  phi <- rep(NA, n_timepoints)
  prev <- rep(0.5, n_timepoints)
  reads <- rep(NA, n_timepoints)
  prev <- rep(NA, n_timepoints)
  prev[c(1, c(1:n_subjects)*days_per_subject)] <- 0.01    

  set.seed(2000)
  for (i in 1:n_timepoints) {
    if (is.na(prev[i])) {
      prev[i] <- reads[i-1]/total[i-1]
    }
    phi[i] <- a + a_subj[ii_subjects[i]] + b_lag * prev[i]
    for (j in 1:n_abx) {
      phi[i] <- (
        phi[i] + 
          b_abxp[ii_subjects[i], j] * abx[i, j] * prev[i] + 
          b_abx[ii_subjects[i], j] * abx[i,j]
      )
    }
    if (rbernoulli(1, p=sparsity)) {
      reads[i] = 0
    } else {
      reads[i] <- rbinom(1, total[i], logistic(phi[i]))      
    }
  }
  
  data <- list(
    n_specimens = n_timepoints,
    n_subjects = n_subjects,
    n_abx = n_abx,
    subjects = ii_subjects,
    abx = abx,
    total = total,
    reads = reads,
    prev = prev
  )
  
  params <- list(
    a = a,
    a_subj = a_subj,
    s_subj = s_subj,
    s_abx = s_abx,
    s_abxp = s_abxp,
    b_lag = b_lag,
    b_abx = b_abx,
    b_abxp = b_abxp
  )
  list(data=data, params=params)
}
set.seed(32)
dsim206 <- with(data.frame(), {
  n_subjects <- 10
  n_abx <- 5
  a <- 0.2
  s_abx <- rexp(n_abx, 1)
  s_abxp <- rexp(n_abx, 1)
  s_subj <- rexp(1, 1)
  a_subj <- rnorm(n_subjects, 0, s_subj);
  b_abx <- t(matrix(rnorm(n_subjects*n_abx, mean=0, sd=s_abx), nrow=n_abx, ncol=n_subjects))
  b_abxp <- t(matrix(rnorm(n_subjects*n_abx, mean=0, sd=s_abxp), nrow=n_abx, ncol=n_subjects))
  b_abx[1,1] <- 1
  b_abxp[1,1] <- 1
  b_lag <- 0.01
  sim206.generator(10, n_subjects, n_abx, a, a_subj, s_subj, s_abx, b_abx, s_abxp, b_abxp, b_lag, 0.1)
})
```

```{r}
m205.4 <- stan(
  file="arb_2.0.5.stan", data=dsim206$data, cores=4, iter=1000, chains=4,
  control=list(max_treedepth=15, adapt_delta=0.9))
saveRDS(m205.4, file="fit_arb_2.0.6_simulated.rds")
```

```{r}
bayesplot::mcmc_trace(
  rstan::extract(m205.4, inc_warmup=T, permuted=F), 
  pars=c("a", "b_abx[1,1]", "s_abx[1]"))
```

```{r}
s205.4 <- summary(m205.4)$summary %>% as.data.frame() %>% 
  rownames_to_column("parameter") %>% as.tibble()
s205.4 %>% filter(Rhat >= 1.1) %>% kable()
```

Sparsity in the outcome variable (reads) provides a lot of the same pathologies as we see in the real data (slow to sample, divergent transitions, treedepth warnings).

Perhaps a hurdle model would sufficiently address this?

```{r}
m206 <- stan(
  file="arb_2.0.6.stan", data=dsim206$data, cores=4, iter=1000, chains=4,
  control=list(max_treedepth=15, adapt_delta=0.9))
saveRDS(m206, file="fit_arb_2.0.6_simulated.rds")
```

Besides sampling extremely slowly, the hurdle model had numerous divergent transitions and treedepth errors. Let's see if using a more abundant taxa helps.

```{r}
d205.5 <- dat205.generator(d2, "Sputum", 2)
m205.5 <- stan(
  file="arb_2.0.5.stan", data=d205.5, cores=4, iter=1000, chains=4,
  control=list(max_treedepth=1, adapt_delta=0.99))

saveRDS(m205.5, file="fit_arb_2.0.5.5_sputum.rds")
```

```{r}
pairs(m205.5, pars=c(
  parameters(m205.5)[grep("^a_sp", parameters(m205.5))][1:201]))
  # "a", "b_lag", "s_spec", "s_subj", "lp__"))
```

## Model v2.0.7

Is the non-centered parameterization hurting things here? Do we actually have "enough" data?

```{r}
m207 <- stan(
  file="arb_2.0.7.stan", data=d205.5, cores=4, iter=1000, chains=4, 
  control=list(max_treedepth=18, adapt_delta=0.8))

```

```{r}
pairs(m207, pars=c(
  parameters(m207)[grep("^a_sp", parameters(m207))][1:10]))
```

This does seem to be the case. There are still numerous treedepth warnings, but no divergences and the Rhat and n_eff values all seem reasonable. Increasing treedepth to 18 eliminates the remaining warnings. Perhaps I was overenthusiastic in non-centered parameterizations for the less sparse data.



## Model v2.0.8

Let's re-add the beta terms.

```{r}
m208 <- stan(
  file="arb_2.0.8.stan", data=d205.5, cores=4, iter=1000, chains=4, 
  control=list(max_treedepth=18, adapt_delta=0.87))
```

No divergences and no treedepth warnings!

## Model v2.0.9

Let's relax our requirement that all subjects must see all antibiotics.

```{r}
dat209.generator <- function(d2, .specimen_type, min_times_abx=1) {
  d3 <- d2 %>% filter(specimen_type==.specimen_type) %>%
    distinct(subject_id, specimen_id2, read_count, total_reads, study_day) %>%
    group_by(subject_id) %>%
    arrange(subject_id, study_day) %>%
    mutate(prev = lag(read_count)/lag(total_reads)) %>%
    right_join(d2) %>%
    filter(!is.na(prev)) %>%
    rename(reads=read_count, total=total_reads, specimens=specimen_id2, subjects=subject_id) %>%
    mutate(abx_b = ifelse(is.na(abx_b), "none", abx_b))
  # browser()
  abx_tally <- d3 %>% group_by(abx_b) %>% 
    summarize(n_subjects = n_distinct(subjects), n_admins=n())
  abx_to_keep <- filter(abx_tally, n_subjects > min_times_abx, abx_b != "none")$abx_b

  subjects_to_keep <- (filter(d3, abx_b %in% abx_to_keep) %>% distinct(subjects))$subjects 

  stopifnot(length(subjects_to_keep) > 0)
  
  d4 <- d3 %>% ungroup() %>%
    filter(subjects %in% subjects_to_keep)
  d_abx <- d4 %>%
    reshape2::dcast(subjects + specimens + reads + total + prev ~ abx_b, value.var="abx_yn", fill=0)
  # browser()
  result <- tidybayes::compose_data(d_abx[, c(1:5)])
  abx <- as.matrix(d_abx[, -c(1:5)])
  # Remove abxs seen less than the minimum, and the NA column
  abx <- abx[, abx_to_keep, drop=F]
  result$abx_b <- as.numeric(as.factor(colnames(abx)))
  result$n_abx <- ncol(abx)
  result$abx <- abx
  result$d4 <- d4
  result
}
d209 <- dat209.generator(d2, "Sputum", 2)
```

It is possible that we need the abx-level beta terms in this model to more completely capture
the "never seen" subject/abx pair. Let's start with just the intercepts for this data (v2.0.7):

```{r}
m209.1 <- stan(
  file="arb_2.0.7.stan", data=d209, cores=4, iter=1000, chains=4, 
  control=list(max_treedepth=18, adapt_delta=0.87))
```

This fit without warnings or divergences. Let's try the model with betas now:

```{r}
m209.2 <- stan(
  file="arb_2.0.8.stan", data=d209, cores=4, iter=2000, chains=4, 
  control=list(max_treedepth=18, adapt_delta=0.8))
saveRDS(m209.2, file="fit_arb_2.0.9.2_sputum.rds")
```

This is taking substantially longer. Let's try it with only `b_abx` (not `b_abxp`).

The fit still took a long time, and there were warnings about low BFMI, as well as parameters that had low n_eff and high Rhat. Specifically, the parameters that didn't mix well had weird energy histograms.

```{r}
pairs(m209.2, pars=c(
  parameters(m209.2)[grep("^a_sp", parameters(m209.2))][1:5]), condition="energy__")
pairs(m209.2,condition="energy__", include=F)
  # parameters(m209.2)[grep("^a_sp", parameters(m209.2))][1:5]), condition="energy__")
```

It seems possible that increasing the chain length would help as it could increase the number of warmup iterations.
However, doubling the length did not remove the BFMI warnings. Could the problem lie in the priors?

```{r}
check_all_diagnostics(m209.2)
launch_shinystan(m209.2)
```

For v2.0.9, I am reparameterizing to remove Cauchy distributions in favor of normal.

```{r}
m209.3 <- stan(
  file="arb_2.0.9.stan", data=d209, cores=4, iter=1000, chains=4, 
  control=list(max_treedepth=18, adapt_delta=0.8))
```

Two chains had BFMI 

## Model v2.0.10

I want to explore adding an abx-specific beta (not subject specific) to see if that helps. I worry however about identifiability.

```{r}
m2010 <- stan(
  file="arb_2.0.10.stan", data=d209, cores=4, iter=1000, chains=4, 
  control=list(max_treedepth=18, adapt_delta=0.8))
```

That doesn't really help anything. Let's check the antibiotic correlations and see if any antibiotics are 
always administered with another:

```{r}
cor(d209$abx)
apply(d209$abx, 2, function(col) {
  swept <- sweep(d209$abx, 1, col) # subtract the column from the rest of the matrix
  swept[swept<0] <- 0
  candidates = colSums(swept) == 0
  colnames(d209$abx)[candidates]
})
```

Nothing there. I guess that's a good thing.

What if we removed the subject-specific effects from the model for now and just used the antibiotic effects?

```{r}
m2010 <- stan(
  file="arb_2.0.10.stan", data=d209, cores=4, iter=1000, chains=4, 
  control=list(max_treedepth=18, adapt_delta=0.8))
```

This fit fine with high enough treedepth (18). It is slow to sample. 

## Model v2.0.11

Let's try a slightly more stringent approach to the antibiotics.

```{r}
dat2011.generator <- function(d2, .specimen_type, min_times_abx=2, min_median_abx=2) {
  d3 <- d2 %>% filter(specimen_type==.specimen_type) %>%
    distinct(subject_id, specimen_id2, read_count, total_reads, study_day) %>%
    group_by(subject_id) %>%
    arrange(subject_id, study_day) %>%
    mutate(prev = lag(read_count)/lag(total_reads)) %>%
    right_join(d2) %>%
    filter(!is.na(prev)) %>%
    rename(reads=read_count, total=total_reads, specimens=specimen_id2, subjects=subject_id) %>%
    mutate(abx_b = ifelse(is.na(abx_b), "none", abx_b))
  # browser()
  abx_to_keep <- (d3 %>% group_by(abx_b, subjects) %>%
    filter(abx_b != "none") %>%
    summarize(n=n()) %>% 
    mutate(n_subjects=n()) %>%
    filter(n_subjects >= min_times_abx, median(n) >= min_median_abx) %>%
    distinct(abx_b))$abx_b
#  abx_to_keep <- filter(abx_tally, n_subjects > min_times_abx, abx_b != "none")$abx_b

  subjects_to_keep <- (filter(d3, abx_b %in% abx_to_keep) %>% distinct(subjects))$subjects 

  stopifnot(length(subjects_to_keep) > 0)
  
  d4 <- d3 %>% ungroup() %>%
    filter(subjects %in% subjects_to_keep)
  d_abx <- d4 %>%
    reshape2::dcast(subjects + specimens + reads + total + prev ~ abx_b, value.var="abx_yn", fill=0)
  # browser()
  result <- tidybayes::compose_data(d_abx[, c(1:5)])
  abx <- as.matrix(d_abx[, -c(1:5)])
  # Remove abxs seen less than the minimum, and the NA column
  abx <- abx[, abx_to_keep, drop=F]
  result$abx_b <- as.numeric(as.factor(colnames(abx)))
  result$n_abx <- ncol(abx)
  result$abx <- abx
  result$d4 <- d4
  result
}
d2011.1 <- dat2011.generator(d2, "Sputum", 4, 3)
```

Going back to the v209 model specification:

```{r}
m2011.1 <- stan(
  file="arb_2.0.9.stan", data=d2011.1, cores=4, iter=1000, chains=4, 
  control=list(max_treedepth=18, adapt_delta=0.8))
```

There are still issues resolving the s_abx parameters. I wonder if a non-centered parameterization would help?

```{r}
m2011.2 <- stan(
  file="arb_2.0.11.stan", data=d2011.1, cores=4, iter=1000, chains=4, 
  control=list(max_treedepth=18, adapt_delta=0.8))
```

That seems to have done it. Centered parameterization for the intercepts and non-centered parameterizations for the betas.

Let's see if this scales to more antibiotics/subjects:

```{r}
d2011.2 <- dat2011.generator(d2, "Sputum", 3, 3)
m2011.3 <- stan(
  file="arb_2.0.11.stan", data=d2011.2, cores=4, iter=3000, chains=4, 
  control=list(max_treedepth=18, adapt_delta=0.87))
saveRDS(m2011.3, "fit_arb_2.0.11.3.rds")
```

There are mixing issues with the chains for some of the sigma terms. Checking the pairs plots and the traces shows one of the chains gets "stuck" in zero-ish modes where the rest of the chains don't go. Increasing the iterations from 1000 to 3000 may permit more mixing?

As it turns out, increasing the iterations leads to divergent transitions. Were we not running enough iterations to detect these divergences before? They all occur out near the edges of the s_abx[5] parameter, which corresponds to vancomyciniv- not exactly a poorly informed antibiotic.

```{r}
pairs(m2011.3, pars=c(
  parameters(m2011.3)[grep("^s_abx", parameters(m2011.3))][1:5], 
  "b_abx[1,5]",
  "b_abx[7,5]",
  "lp__"))
```



