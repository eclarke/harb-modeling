---
title: "Hierarchical, autoregressive binomial models for MIRA 16S data"
author: "Erik Clarke"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: github_document
editor_options: 
  chunk_output_type: console
params:
  fig_fp: "figures"
  genus: "Pseudomonas"
  abx: "cefepime"
---

Click on "Code" for information on packages loaded for this analysis.
```{r setup, collapse=TRUE}
library(here)
library(tidyverse); print(packageVersion("tidyverse"))
library(phyloseq)
library(rethinking)
library(bayesplot)
library(tidybayes)
library(ggbeeswarm)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

knitr::opts_chunk$set(
  echo = TRUE,
  results="hide",
  warning=FALSE,
  message = FALSE,
  fig.width=5,
  fig.height = 3)

fig_fp <- params$fig_fp
if (!dir.exists(here(fig_fp))) {
  dir.create(here(fig_fp))
}

source(here("shared_functions.R"))

mira_theme <- theme_bw() + theme(
  strip.background = element_rect(fill = NA)
)
theme_set(mira_theme)
```

## Data

For these models, we are aggregating counts and considering only the `r params$genus` genus and `r params$abx` antibiotic.
The response variable will be the raw read counts, while the treatment will be a binary yes/no for
`r params$abx` administration on the previous day (either that or no abx), and the previous day's read counts for 
`r params$genus` (standardized).

```{r load-data, cache=TRUE}
mira <- load_mira_data(
  seqtab_fp = here("../shared/data/seqtab.rds"),
  taxa_fp = here("../shared/data/taxa.rds"),
  specimen_fp = here("../shared/data/MIRA_Specimen_Table.csv")
)
seqs <- mira$seqs
mira <- mira$ps

# Keep only the samples with abx data
mira.abx <- prune_samples(!is.na(sample_data(mira)$abx_b_number), mira)
# Converted to melted form
agg <- phyloseq_to_agglomerated(mira.abx, "specimen_id2", "otu_id", "read_count")
d <- agg %>%
  filter(specimen_type %in% c("Stool Swab", "Oral Swab", "Sputum")) %>%
  # Calculate total reads
  group_by(specimen_id2) %>%
  mutate(total_reads = sum(read_count)) %>%
  ungroup() %>%
  # Collapse reads by genus
  filter(!is.na(Genus)) %>%
  group_by(specimen_id2, total_reads, Kingdom, Phylum, Class, Order, Family, Genus) %>%
  summarise(read_count = sum(read_count)) %>%
  filter(Genus == params$genus) %>%
  left_join(sample_data(mira.abx)) %>%
  # Isolate to instances of target antibiotic
  ungroup() %>%
  separate_rows(abx_b) %>%
  filter(abx_b == params$abx | is.na(abx_b)) %>%
  # Set up lag variables, removing days with > 1 day since previous sampling
  group_by(specimen_type, subject_id) %>%
  arrange(study_day) %>%
  mutate(day_delta = study_day - lag(study_day)) %>%
  mutate(lag_count = lag(read_count)) %>%
  mutate(lag_abx_yn = ifelse(is.na(lag(abx_b)), 0, 1)) %>%
  ungroup() 
  
```

```{r shared-functions}
run_over_specimen_types <- function(d, formula, chains=2, iter=2000, ...) {
  run_model <- function(flist, data, model, ...) {
    if (missing(flist)) {
      if (missing(model)) {
        stop("Must give either a flist or model")
      }
      map2stan(
        flist=model,
        data=as.data.frame(data),
        ...
      )
    } else {
      map2stan(
        flist=flist,
        data=as.data.frame(data),
        ...
      )
    }

  } 
  
  flist=NULL
  models <- list()
  for (type in c("Stool Swab", "Oral Swab", "Sputum")) {
    d2 <- filter(d, specimen_type == type)# %>%
      # select(read_count, total_reads, specimen_id, subject_id, lag_count_s, lag_abx_yn)
    if (is.null(flist)) {
      flist <- formula
      m <- run_model(flist=flist, data=d2, chains=chains, iter=iter, ...)
      flist <- m
      print(type)
      print(precis(m))
      models[[type]] <- m
    } else {
      print("Skipping model compilation")
      m <- run_model(data=d2, model=flist, chains=chains, iter=iter, ...)
      print(type)
      print(precis(m))
      models[[type]] <- m
    }
  }
  models
}
```


## Model 1: Binomial model with specimen and subject intercepts

### Model 1.0

Random intercepts for specimen and subject.

```{r, results="hide"}

f1.0 <- alist(
  read_count ~ dbinom(total_reads, prob),
  logit(prob) <- a_0 + a_specimen[specimen_id2] + a_subject[subject_id],
  a_0 ~ dnorm(0,10),
  a_specimen[specimen_id2] ~ dnorm(0, sigma_specimen),
  a_subject[subject_id] ~ dnorm(0, sigma_subject),
  sigma_specimen ~ dcauchy(0,1),
  sigma_subject ~ dexp(1)
)

m1.0 <- d %>% 
  run_over_specimen_types(
    f1.0, iter = 4000, chains=4, cores=4, WAIC=F, control=list(adapt_delta=0.9, max_treedepth=12))
```

### Model 1.1

Single-subject, random intercepts on specimen, autoregressive on previous day.

```{r m1.1}
d1.1 <- as.list(d %>% filter(
  subject_id == "MIRA_013", specimen_type=="Sputum") %>%
  select(read_count, total_reads, specimen_id2)  %>%
  mutate(specimen_id2 = as.integer(as.factor(specimen_id2))) %>%
  mutate(emp_prob = read_count/total_reads))
d1.1$N <- length(d1.1$read_count)
d1.1$N_specimen_id2 <- n_distinct(d1.1$specimen_id2)
writeLines(readLines("arb_1.1.stan"))
m1.1 <- stan(file="arb_1.1.stan", data=d1.1, iter=5000, warmup=1000, control=list(max_treedepth=15, adapt_delta=0.9))

```

### Model 1.2

Same as 1.1, but now also with a binary predictor for `r params$abx`.

```{r m1.2}
d1.2 <- d %>% filter(
  subject_id == "MIRA_005", specimen_type=="Oral Swab") %>%
  mutate(abx_yn = !is.na(abx_b)) %>%
  select(read_count, total_reads, abx_yn, study_day, specimen_id2)  %>%
  mutate(specimen_id2 = as.factor(specimen_id2)) %>%
  mutate(study_day = study_day) %>%
  mutate(emp_prob = read_count/total_reads)

writeLines(readLines("arb_1.2.stan"))

m1.2 <- stan(
  file="arb_1.2.stan", data=compose_data(d1.2), iter=5000, chains=3,
  control=list(max_treedepth=15, adapt_delta=0.99))

```

```{r}
post1.2 <- spread_samples(m1.2.3, a_0, b_lag, b_abx) 
pred1.2 <- matrix(data=NA, nrow = 7500, ncol=nrow(d1.2))
emp_pred1.2 <- pred1.2
pred1.2[,1] <- d1.2$read_count[1]
emp_pred1.2[,1] <- d1.2$emp_prob[1]
for (i in 2:nrow(d1.2)) {
  pred1.2[,i] <- with(
    post1.2, 
    rbinom(7500, size=d1.2$total_reads[i], 
           prob=logistic(a_0 + b_lag*(pred1.2[,i-1]/d1.2$total_reads[i-1]) + b_abx*d1.2$abx_yn[i-1])))
  emp_pred1.2[,i] <- pred1.2[,i]/d1.2$total_reads[i]
}
colnames(emp_pred1.2) <- paste0("X", d1.2$study_day)
pred1.2.df <- gather(data.frame(emp_pred1.2))
pred1.2.df$key <- as.numeric(str_replace(pred1.2.df$key, "X", ""))
pred1.2.df <- group_by(pred1.2.df, key) %>%
  mutate(sample = seq_along(value))
pred1.2.df <- filter(pred1.2.df, sample < 100)

ggplot(pred1.2.df, aes(key, value, group=sample)) + geom_line(alpha=0.1) +
  geom_line(data=d1.2, aes(study_day, emp_prob, color="empirical", group=NULL)) +
  geom_vline(data=d1.2, aes(xintercept=study_day, alpha=abx_yn), linetype=2) +
  scale_alpha_manual(values=c("TRUE"=0.6, "FALSE"=0)) +
  scale_y_continuous(expand=c(0,0), labels=scales::percent)
```

### Model 1.2.1

Alternative parameterization of the half-Cauchy

- exp(1)

```{r}
.m <- map2stan(alist(
  read_count ~ dbinom(total_reads, prob),
  logit(prob) <- a_0 + a_specimen[specimen_id2],
  a_0 ~ dnorm(0,10),
  a_specimen[specimen_id2] ~ dnorm(0, sigma_specimen),
  sigma_specimen ~ dexp(1)
), data=data.frame(d1.2), WAIC=F)
```


```{r m1.2}

writeLines(readLines("arb_1.2.1.stan"))

m1.2.1 <- stan(
  file="arb_1.2.1.stan", data=compose_data(d1.2), iter=5000, chains=3,
  control=list(max_treedepth=15, adapt_delta=0.99))
m1.2.2 <- stan(
  file="arb_1.2.2.stan", data=compose_data(d1.2), iter=5000, chains=3,
  control=list(max_treedepth=15, adapt_delta=0.99))
# 1.2, but with transformed cauchy
m1.2.3 <- stan(
  file="arb_1.2.3.stan", data=compose_data(d1.2), iter=5000, chains=3,
  control=list(max_treedepth=15, adapt_delta=0.99))
```

## Model 1.3

Revisiting the intercept-only model to explore better parameterizations.

This goes right back to the start with no separate intercept `a_0`, just specimen-level intercepts.
```{r}
d1.3 <- d %>% filter(
  subject_id == "MIRA_005", specimen_type=="Oral Swab") %>%
  mutate(abx_yn = !is.na(abx_b)) %>%
  select(read_count, total_reads, abx_yn, study_day, specimen_id2)  %>%
  mutate(specimen_id2 = as.factor(specimen_id2)) %>%
  mutate(study_day = study_day) %>%
  mutate(emp_prob = read_count/total_reads)

writeLines(readLines("arb_1.2.stan"))

m1.3.0 <- stan(
  file="arb_1.3.0.stan", data=compose_data(d1.3), iter=5000, chains=3)
#  control=list(max_treedepth=15, adapt_delta=0.9))

```

### Model 1.3.1

This one changes things up a little. We use non-centered intercepts for the specimens and add back the lag parameter.

The tree depth is important here (max = 15).

```{r}

m1.3.1 <- stan(
  file="arb_1.3.1.stan", data=compose_data(d1.3), iter=5000, chains=3,
  control=list(max_treedepth=15))
```

### Model 1.3.2

Do centered vs non-centered parameterizations matter? This model uses centered parameterizations:

```{r}

m1.3.2 <- stan(
  file="arb_1.3.2.stan", data=compose_data(d1.3), iter=5000, chains=3,
  control=list(max_treedepth=15))
```

`n_eff` is generally higher in m1.3.1, indicating that non-centered parameterization does indeed lead to 
a bit more efficient sampler. 

### Model 1.3.3 

The return of antibiotic effects! And non-centered parameterization.
Notes: I had to use `vector[n] abx_yn` rather than a better `int abx_yn[n]` due to 
the vector slicing notation we're using.

```{r}
m1.3.3 <- stan(
  file="arb_1.3.3.stan", data=compose_data(d1.3), iter=5000, chains=3,
  control=list(max_treedepth=15))
```

I've added a 'generated quantities' block that converts back from alpha_std to alpha, 
and these values recapitulate what I got from m1.3.2 (the centered version).

Okay. So this is the "gold standard" for now. 

## Model 1.4

In this model I want to explore subject-level random effects. It should be as easy as 
adding a second non-centered reparameterization around a second alpha, right?

For sanity's sake, I'm removing the lag and antibiotic terms for now.

```{r}
d1.4 <- d %>% filter(
  subject_id %in% c("MIRA_005", "MIRA_004", "MIRA_007"), specimen_type=="Oral Swab") %>%
  mutate(abx_yn = !is.na(abx_b)) %>%
  select(read_count, total_reads, abx_yn, study_day, subject_id, specimen_id2)  %>%
  mutate(specimen_id2 = as.factor(specimen_id2)) %>%
  mutate(subject_id = as.factor(droplevels(subject_id))) %>%
  mutate(study_day = study_day) %>%
  mutate(emp_prob = read_count/total_reads)

m1.4.0 <- stan(
  file="arb_1.4.0.stan", data=compose_data(d1.4), iter=2000, chains=4,
  control=list(max_treedepth=15, adapt_delta=0.9))

bayesplot::mcmc_trace(extract(m1.4.0, inc_warmup=F, permuted=F), pars=c("mu", "sigma_spec", "sigma_subj", "a_spec[1]"))
pairs(m1.4.0, pars=c("mu", "sigma_spec", "sigma_subj", "a_spec[1]"))
```

Couple of things to note: 

- Though I expected it to be a 1-to-1 relationship, putting the transformed random intercepts in the 
transformed parameters block led to serious divergences. I'm keeping them outside that block now, in the model block.
- I've used exponential priors for the sigma terms here but I'm not sure they're necessary. 
- This samples relatively efficiently.

```{r}
invisible(with(data.frame(), {
  browser()
  post <- tidybayes::spread_samples(model=m1.4.0, a_subj[subject_id], sigma_subj) %>%
    group_by(subject_id)
  .d <- d1.4 %>% mutate(subject_id = as.integer(subject_id))
  ggplot(post, aes(subject_id, logistic(a_subj), group=subject_id)) + geom_quasirandom(shape=21) +
    geom_quasirandom(data=.d, aes(y=emp_prob))
}))

```

This shows the subject-level intercepts reasonably reconstituting the empirical data. They're all
pulled towards zero, which I expect is closer to the median thanks to the one subject of the three
that's mostly zero.

### Model 1.4.1

Let's explore now how to add a lag term to this model. First I'll start by rewriting it
to be "ragged"; i.e. each subject's terms are fit separately. 

```{r}
d1.4.1 <- d1.4 %>% arrange(subject_id, study_day) %>% compose_data()
d1.4.1$n_obs_subject <- (d1.4 %>% group_by(subject_id) %>% summarize(n = n()))$n

m1.4.1 <- stan(
  file="arb_1.4.1.stan", data=d1.4.1, iter=2000, chains=4,
  control=list(max_treedepth=15, adapt_delta=0.9))
```

This model works about as well as 1.4.0. The estimates are very similar.

### Model 1.4.2

Now that we have a for-loop over each subject, we should be able to add the 
correct lag term.

```{r}
m1.4.2 <- stan(
  file="arb_1.4.2.stan", data=d1.4.1, iter=2000, chains=4,
  control=list(max_treedepth=15, adapt_delta=0.9))

bayesplot::mcmc_trace(extract(m1.4.2, inc_warmup=F, permuted=F), pars=c("mu", "sigma_spec", "sigma_subj", "a_spec[1]", "b_lag"))
pairs(m1.4.2, pars=c("mu", "sigma_spec", "sigma_subj", "a_spec[1]", "b_lag"))
```

This model converges fine and seems reasonable. I'm not simulating from the posterior yet.

### Model 1.4.3

Let's add a subject-level effect for antibiotics without pooling (for now).
My intuition for this will be that it fits slower, and has a hard time converging.
I may have been better off estimating b_abx with complete pooling.

```{r}
m1.4.3 <- stan(
  file="arb_1.4.3.stan", data=d1.4.1, iter=2000, chains=4,
  control=list(max_treedepth=17, adapt_delta=0.95))

bayesplot::mcmc_trace(extract(m1.4.3, inc_warmup=F, permuted=F), pars=c("mu", "sigma_spec", "sigma_subj", "a_spec[1]", "b_lag", "b_abx[1]"))
pairs(m1.4.3, pars=c("mu", "sigma_spec", "sigma_subj", "a_spec[1]", "b_lag", "b_abx[1]"))
```

This converges with 3 divergent iterations and no treedepth warnings.

### Model 1.4.4

Partial pooling on b_abx.

```{r}
m1.4.4 <- stan(
  file="arb_1.4.4.stan", data=d1.4.1, iter=2000, chains=4,
  control=list(max_treedepth=17, adapt_delta=0.95))

bayesplot::mcmc_trace(extract(m1.4.4, inc_warmup=F, permuted=F), pars=c("mu", "sigma_spec", "sigma_subj", "a_spec[1]", "b_lag", "b_abx[1]"))
pairs(m1.4.4, pars=c("mu", "sigma_spec", "sigma_subj", "a_spec[1]", "b_lag", "b_abx[1]"))
```

This fits okay on the restricted 3-subject dataset, so let's try it on the full dataset:

```{r}
d1.4.4.os <- d %>% filter(specimen_type=="Oral Swab") %>%
  group_by(subject_id) %>%
  filter(n() > 1) %>%
  ungroup() %>%
  mutate(abx_yn = !is.na(abx_b)) %>%
  select(read_count, total_reads, abx_yn, study_day, subject_id, specimen_id2)  %>%
  mutate(specimen_id2 = as.factor(specimen_id2)) %>%
  mutate(subject_id = as.factor(droplevels(subject_id))) %>%
  mutate(study_day = study_day) %>%
  mutate(emp_prob = read_count/total_reads)
d1.4.4.osl <- d1.4.4.os %>% arrange(subject_id, study_day) %>% compose_data()
d1.4.4.osl$n_obs_subject <- (d1.4.4.os %>% group_by(subject_id) %>% summarize(n = n()))$n

m1.4.4a <- stan(
  file="arb_1.4.4.stan", data=d1.4.4.osl, iter=2000, chains=4,
  control=list(max_treedepth=17, adapt_delta=0.95))
saveRDS(m1.4.4a, file = "fit_arb_1.4.4_oralswab.rds")
```

It fits! Took about an hour or more to do so.

#### Fitting on other sample types

```{r}
d1.4.4.sp <- d %>% filter(specimen_type=="Sputum") %>%
  group_by(subject_id) %>%
  filter(n() > 1) %>%
  ungroup() %>%
  mutate(abx_yn = !is.na(abx_b)) %>%
  select(read_count, total_reads, abx_yn, study_day, subject_id, specimen_id2)  %>%
  mutate(specimen_id2 = as.factor(specimen_id2)) %>%
  mutate(subject_id = as.factor(droplevels(subject_id))) %>%
  mutate(study_day = study_day) %>%
  mutate(emp_prob = read_count/total_reads) %>%
  ungroup() %>%
  arrange(subject_id, study_day)
d1.4.4.spl <- compose_data(d1.4.4.sp)
d1.4.4.spl$n_obs_subject <- (d1.4.4.sp %>% group_by(subject_id) %>% summarize(n = n()))$n

m1.4.4.sp <- stan(
  file="arb_1.4.4.stan", data=d1.4.4.spl, iter=2000, chains=4,
  control=list(max_treedepth=17, adapt_delta=0.95))
saveRDS(m1.4.4.sp, file = "fit_arb_1.4.4_sputum.rds")
```


## Model 1.5

After all the difficulties around 1.4 and vector slicing, I realized that simply by specifying the lagged values
in the data, we'd recapitulate what we're doing with the slicing operations. Stan is not learning from the 
previous day's fit, it's learning from the previous day's data just as if we added a lagged value. 

Plus, we would like to add lead and lag times to antibiotic effectiveness. It shouldn't start immediately,
and shouldn't wear off immediately.

So let's skip all that and try this again.

### Model 1.5.0

```{r}
d1.5.0.sp <- d %>% filter(specimen_type == "Sputum") %>%
  select(subject_id, specimen_id2, study_day, total_reads, read_count, abx_b) %>%
  # Remove empty or single-sample subjects
  group_by(subject_id) %>%
  filter(n() > 1) %>%
  arrange(subject_id, study_day) %>%
  # Establish abx effectiveness periods:
  # 1) Translate abx to T/F
  mutate(abx_yn = !is.na(abx_b)) %>%
  # Fill in missing sampling days
  tidyr::complete(study_day=full_seq(study_day, 1)) %>%
  # Establish window for delayed onset and delayed loss
  mutate(on_abx = effective_window(abx_yn, lag=2, lead=3)) %>%
  # Add lagged read and empirical proportion terms %>%
  mutate(lag_count = lag(read_count)) %>%
  mutate(lag_emp_prop = lag(read_count)/lag(total_reads)) %>%
  # Remove missing cases 
  filter(!is.na(lag_count) & !is.na(read_count)) %>%
  ungroup() %>%
  # select(subject_id, specimen_id2, total_reads, read_count, lag_emp_prop, on_abx) %>%
  droplevels()
d1.5.0.spl <- compose_data(d1.5.0.sp)

m1.5.0.sp <- stan(
  file="arb_1.5.0.stan", data=d1.5.0.spl, iter=2000, chains=4,
  control=list(max_treedepth=15, adapt_delta=0.9))
saveRDS(m1.5.0.sp, file="fit_arb_1.5.0_sputum.rds")
bayesplot::mcmc_trace(extract(m1.5.0.sp, inc_warmup=F, permuted=F), pars=c("mu", "sigma_spec", "sigma_subj", "a_spec[1]", "b_lag"))
pairs(m1.5.0.sp, pars=c("mu", "sigma_spec", "sigma_subj", "a_spec[1]", "b_lag", "b_abx[1]"))
```

#### Post. predictive

```{r}
invisible(with(data.frame(), {
  browser()
  .m <- m1.5.0.sp
  .d <- d1.5.0.sp
  .m <- recover_types(.m, .d)
  post <- spread_samples(.m, mu, sigma_spec, sigma_subj, sigma_abx, b_lag, a_spec[specimen_id2], a_subj[subject_id], b_abx[subject_id])
  post$specimen_idx <- as.integer(post$specimen_id2)
  post2 <- left_join(post, .d) %>% filter(!is.na(read_count))
  
  pp_subject <- function(post, subject, use_a_spec=F) {
    ps <- post %>% filter(subject_id == subject)
    stopifnot(nrow(ps) > 0)
    ps$specimen_idx <- as.integer(as.factor(ps$specimen_id2))
    cols <- n_distinct(ps$specimen_id2)
    stopifnot(cols > 1)
    rows <- nrow(ps)/cols
    pred_prob <- matrix(data=NA, nrow=rows, ncol=cols) 
    
    
    p.link <- function(d, n, idx, prev_prop) {
      d2 <- filter(d, specimen_idx == idx)
      prob <- logistic(with(d2, mu + a_subj + a_spec*use_a_spec + b_lag * prev_prop + b_abx * on_abx))
      rbinom(n, unique(d2$total_reads), prob)/unique(d2$total_reads)
    }
    
    pred_prob[,1] <- p.link(ps, rows, 1, .d$lag_emp_prop[1])
    
    for (i in 2:cols) {
      pred_prob[,i] <- p.link(ps, rows, i, pred_prob[,i-1])
    }
    
    pred.df <- gather(data.frame(pred_prob)) %>%
      mutate(key = as.factor(key))
    levels(pred.df$key) <- levels(as.factor(ps$specimen_id2))
    
    pred.df.sub <- group_by(pred.df, key) %>%
      mutate(group = seq_along(value)) %>%
      filter(group < 150) %>%
      rename(specimen_id2=key, pred_prob=value) %>%
      left_join(.d) %>%
      mutate(emp_prob = read_count/total_reads)
    
    dp = pred.df.sub %>% ungroup %>% distinct(specimen_id2, .keep_all = T)
    
    ggplot(pred.df.sub, aes(study_day, pred_prob, group=group)) +
      geom_line(alpha=0.1) +
      geom_line(data=dp, aes(y=emp_prob, group=NULL), color="red") +
      geom_vline(data=dp, aes(xintercept=study_day, alpha=on_abx), linetype=2) +
      scale_alpha_manual(sprintf("on %s", params$abx), values=c("TRUE"=0.6, "FALSE"=0)) +
      scale_y_continuous(expand=c(0,0), labels=scales::percent) +
      ggtitle(subject)
  }
  
  pp_subject_plots <- sapply(X = as.character(unique(post$subject_id)), function(s) {
    print(s)
    pp_subject(post2, s)
  })
  

}))
```

